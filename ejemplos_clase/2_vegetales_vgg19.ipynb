{"cells":[{"cell_type":"markdown","source":["<a href=\"https://www.inove.com.ar\"><img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/PA%20Banner.png\" width=\"1000\" align=\"center\"></a>\n","\n","\n","# Ejercicio de clasificación con redes neuronales convolucionales (CNN)\n","\n","Ejemplo de clasificación utilizando redes neuronales convolucionales para la clasificación de imagenes<br>\n","\n","v1.1"],"metadata":{"id":"hFwa0fhoKqlG"}},{"cell_type":"markdown","source":["### **Objetivos:**\n","* Estudiar el dataset de Vegetales.\n","* Implementar Transfer Learning y Redes Convolucionales para la clasificación de imágenes de los Vegetales."],"metadata":{"id":"oJM1rCXmLVSt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4O4B39e83u3"},"outputs":[],"source":["# Librerías a implementar\n","import json \n","import zipfile\n","import os\n","import pandas as pd\n","from PIL import Image\n","import random\n","\n","import matplotlib.image as mpimg\n","from glob import glob\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QV81z0zx87Zq"},"outputs":[],"source":["root_path = '/content'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqlzTzUI9C6r"},"outputs":[],"source":["!pip install kaggle\n","!mkdir ~/.kaggle\n","!touch ‘/root/.kaggle/kaggle.json’\n","\n","###############################################\n","# IMPORTANTE \n","# Copia en la siguiente línea el contenido de tu archivo kaggle.json \n","api_token = {'username':'johanarangel','key':'1bbca8a2d35b2e4dfa24534a5153e10b'}\n","\n","###############################################\n","with open('/root/.kaggle/kaggle.json', 'w') as file:\n"," json.dump(api_token, file)\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2EDqfBtD-UhP"},"outputs":[],"source":["!kaggle datasets download -d misrakahmed/vegetable-image-dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwka5JSD-gU0"},"outputs":[],"source":["# Ahora procedamos a descomprimir las carpetas que contienen las imágenes .\n","for file in os.listdir():\n"," if file.endswith('.zip'):\n","  zip_ref = zipfile.ZipFile(file, 'r')\n","  zip_ref.extractall()\n","  zip_ref.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOAqJAGZAVVj"},"outputs":[],"source":["# Visualizar los directiorios.\n","os.listdir(\"./Vegetable Images\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYb3krC9A4gS"},"outputs":[],"source":["# /content/Vegetable Images/test\n","# /content/Vegetable Images/train\n","# /content/Vegetable Images/validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNkmocKrAzUe"},"outputs":[],"source":["# Visualizar los tipos de vegetales\n","train_dir = \"./Vegetable Images/train\"\n","validation_dir = \"./Vegetable Images/validation\"\n","os.listdir(train_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwYDwWbEBavk"},"outputs":[],"source":["vegetales = os.listdir(train_dir)\n","print(\"Cantidad de tipos de vegetales:\", len(vegetales))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edbJ-oduHudY"},"outputs":[],"source":["vegetales"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WNAe-WUETfo"},"outputs":[],"source":["# Imagenes de la primer carpeta, que esta en la primer ubicación\n","files = glob(train_dir + \"/\" + vegetales[0] + \"/**.jpg\")\n","#files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vA2Zr_whCPq3"},"outputs":[],"source":["# Visualizar las 10 primeras imagenes de un vegetal\n","# glob(), encuentra todos los nombres de rutas que se asemejan \n","# a un patrón especificado de acuerdo a las reglas que sigue la ruta.\n","files = glob(train_dir + \"/\" + vegetales[0] + \"/**.jpg\")\n","\n","# plt alias de Matplotlib.\n","# Método figure() crea el espacio para dibujar.\n","# Con figsize=(16,9) se define el ancho y alto del dibujo\n","fig = plt.figure(figsize=(16,9))\n","\n","# Bucle que itera 10 veces para mostrar las primeras 10 imágenes del dataset\n","for i in range(12):\n","\n","    # ax gráfico que mostrará las imágenes en 2 filas y 5 columnas\n","    # En cada iteración va ubicando la imagen en la siguiente posición (i+1)\n","    ax = fig.add_subplot(3, 4, i+1)\n","    # .axis('off') elimina el recuadro de cada imagen\n","    ax.axis('off')\n","    # Herramienta de Matplotlib para para leer imágenes\n","    img = mpimg.imread(files[i])\n","    # Muestra las imágenes de la variable data_X_train en el espacio del dibujo\n","    plt.imshow(img)\n","\n","# Muestra la figura\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6EQf4ZAFc28"},"outputs":[],"source":["# Visualizar la dimension de la primera imagen\n","img = mpimg.imread(files[0])\n","img.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1YvMoghF521"},"outputs":[],"source":["# Visualizar como están representados los pixeles internos.\n","print(img[85, 100:110, :])"]},{"cell_type":"markdown","metadata":{"id":"q0dSbpopGMe0"},"source":["#### Conclusiones\n","- Las imagenes tienen todos el mismo tamaño variable, 224x224, que es importante ya que el clasificador vgg19 recibe imágenes del tamaño indicado.\n","- Las imagenes están representadas de 0 a 255, hay que normalizarlas"]},{"cell_type":"markdown","source":["## Observar que tan balanceado está el dataset"],"metadata":{"id":"ZArey6KROTbV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QBWxnNhGwWK"},"outputs":[],"source":["# Analizar cuantos vegetales hay de cada uno\n","nombres_vegetales = []\n","cantidad_vegetal = []\n","\n","# Bucle que itera la lista de vegetales\n","for vegetal in vegetales:\n","    \n","    # Almacena la ruta completa de cada imagen \n","    files = glob(train_dir + \"/\" + vegetal + \"/**.jpg\")\n","    \n","    # Por cada nombre de imagen separa por _\n","    nombre_vegetal= vegetal.split(\"_\")[0]\n","\n","    # Almacena el nombre de cada vegetal en una lista\n","    nombres_vegetales.append(nombre_vegetal)\n","\n","    # Almacena la cantidad de rutas que es igual a la cantidad de vegetales\n","    cantidad_vegetal.append(len(files))\n","\n","# Graficar la cantidad de imágenes que tiene cada vegetal\n","# plt alias de Matplotlib.\n","# Método figure() crea el espacio para dibujar.\n","# Con figsize=(16,9) se define el ancho y alto del dibujo\n","fig = plt.figure(figsize=(16,9))\n","\n","# espacio ax para el gráfico a mostrar\n","ax = fig.add_subplot()\n","\n","# Gráfico de barra (barplot)\n","# sns, alias de Seaborn\n","# ax=ax, los datos se representarán en horiizontal\n","sns.barplot(x=nombres_vegetales, y=cantidad_vegetal, ax=ax)\n","\n","# Mostrar la imagen.\n","plt.show()"]},{"cell_type":"markdown","source":["El dataset está perfectamente balanceado."],"metadata":{"id":"nQDBDqgFPzPu"}},{"cell_type":"markdown","metadata":{"id":"CH5I7Ldy8_a8"},"source":["# Procesar datos\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline2.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNArKWc38-pg"},"outputs":[],"source":["# Se importa ImageDataGenerator del módulo de keras.preprocessing.image\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","# Crear un generador, indicando si deseamos realizar un escalado de la imagen\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# El método .flow_from_directory, toma la ruta a un directorio y genera lotes de datos aumentados.\n","# target_size, se indica la dimensión de la imagen que se desea.\n","# batch_size, la cantidad que va a tomar para aplicar la operación de escalado.\n","# class_mode, es categorical ya que son varios vegetales.\n","train_generator = train_datagen.flow_from_directory(\n","        directory=train_dir,\n","        target_size=(224, 224),\n","        batch_size=140,\n","        class_mode=\"categorical\")\n","\n","# Mismo proceso para los datos de validación \n","validation_generator = validation_datagen.flow_from_directory(\n","        directory=validation_dir,\n","        target_size=(224, 224),\n","        batch_size=80,\n","        class_mode=\"categorical\")\n","\n","# Con dict, arma un diccionario\n","# con zip, es una función toma que iterables como argumentos y devuelve un iterador.\n","# Es decir, se construye en diccionario indice:valor --> ubicacion:nombre_vegetal\n","index_to_classes = dict(zip(train_generator.class_indices.values(), train_generator.class_indices.keys()))\n","index_to_classes "]},{"cell_type":"markdown","metadata":{"id":"rgcOxcu5UUqZ"},"source":["# Explorar datos\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline3.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Al5AUemQUgXx"},"outputs":[],"source":["# El generador \"train_generator\" se lo puede utilizar para acceder a los datos\n","# de a cantidad batch de imagenes. En este caso el generador me retornará\n","# la primera vez las primeras 20 imagenes\n","# El generador devuelve las imagenes (X) y las clases(personaes) a las que\n","# pertenece (y)\n","# X, y = train_generator.next()\n","batch_imagenes, batch_clases = train_generator.next()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BaQbbz08UnF-"},"outputs":[],"source":["# Cantidad de imágenes, dimensión alto, dimensión ancho, canales de color\n","batch_imagenes.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJ3TpH3xUsey"},"outputs":[],"source":["# Cantidad de imagenes y categorías\n","batch_clases.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JN9SErLLVP2q"},"outputs":[],"source":["# Observar las primeras 5 imagenes de ese batch\n","# plt alias de Matplotlib.\n","# Método figure() crea el espacio para dibujar.\n","# Con figsize=(16,9) se define el ancho y alto del dibujofig = plt.figure(figsize=(16,9))\n","# Observar las primeras 5 imagenes de ese batch\n","fig = plt.figure(figsize=(16,9))\n","\n","# Itera 5 veces\n","for i in range(10):\n","\n","    # ax, gráfico que mostrará las imágenes en 1 filas y 5 columnas\n","    # En cada iteración va ubicando la imagen en la siguiente posición (i+1)\n","    ax = fig.add_subplot(2, 5, i+1)\n","\n","    # Muestra la imagen\n","    ax.imshow(batch_imagenes[i])\n","\n","    # Ubica por la posición de la imagen el nombre que le corresponde.\n","    numero_clase = batch_clases[i].argmax()\n","\n","    # A cada imagen le agrega un titulo que sería el nombre del vegetal que le corresponde.\n","    ax.set_title(index_to_classes[numero_clase])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"37oRA7WDVtfW"},"source":["__Importante__! Luego de usar un generador \"jugando\", ese batch de imagenes que sacamos ya no se encontrará disponible para ser utilizado en el entrenamiento, es recomendable volver a crear los generadores si se los consumen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AJKYkWJV2Q1"},"outputs":[],"source":["# Crear un generador, indicando si deseamos realizar un escalado de la imagen\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","\n","\n","# El método .flow_from_directory, toma la ruta a un directorio y genera lotes de datos aumentados.\n","# target_size, se indica la dimensión de la imagen que se desea.\n","# batch_size, la cantidad que va a tomar para aplicar la operación de escalado.\n","# class_mode, es categorical ya que son varios vegetales.\n","train_generator = train_datagen.flow_from_directory(\n","        directory=train_dir,\n","        target_size=(224,224),\n","        batch_size=140,\n","        class_mode=\"categorical\")\n","\n","# Con dict, arma un diccionario\n","# con zip, es una función toma que iterables como argumentos y devuelve un iterador.\n","# Es decir, se construye en diccionario indice:valor --> ubicacion:nombre_vegetal\n","index_to_classes = dict(zip(train_generator.class_indices.values(), train_generator.class_indices.keys()))"]},{"cell_type":"markdown","metadata":{"id":"QDGq9EnAWKU4"},"source":["# Entrenar modelo\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline4.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"markdown","metadata":{"id":"1ryYNpBHWN1e"},"source":["###NOTE: Los generadores ya se encargan de transformar la salida a oneHotEncoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOJkmYGHWTF3"},"outputs":[],"source":["# input shape (observado del análisis de datos)\n","# Almacena las dimensiones y los canales de color, sería la entrada a la red\n","in_shape = (224, 224, 3)\n","print('Entrada', in_shape)\n","\n","# output shape (observado del análisis de datos)\n","# 42 ya que representa categorías, los nombres de los vegetales con lo que se entrena la red\n","out_shape = 15\n","print('Salida:', out_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFSEqhgTWxtg"},"outputs":[],"source":["# Debemos definir cuantas imagenes se consumiran por epoca para entrenamiento \n","# y validación (steps_per_epoch).\n","# ya que estando el generador en el medio Keras no puede saberlo por\n","# su cuenta\n","epoch_train = len(train_generator)\n","print(epoch_train)\n","\n","epoch_validation = len(validation_generator)\n","print(epoch_validation )"]},{"cell_type":"markdown","metadata":{"id":"amYbNCHNck3y"},"source":["\n","VGG-19 es una red neuronal convolucional con 19 capas de profundidad. Puede cargar una versión preentrenada de la red entrenada en más de un millón de imágenes desde la base de datos [1] de ImageNet. La red preentrenada puede clasificar imágenes en 1000 categorías de objetos (por ejemplo, teclado, ratón, lápiz y animales). Como resultado, la red ha aprendido representaciones ricas en características para una amplia gama de imágenes. El tamaño de la entrada de imagen de la red es de 150 por 150. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTNigZWwaG6u"},"outputs":[],"source":["# Fuente = https://keras.io/api/applications/#usage-examples-for-image-classification-models\n","from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Model\n","\n","modelo_vgg = VGG19(weights='imagenet',\n","                   input_shape=in_shape,  \n","                   include_top=False,     # No incluir las 3 capas completamente conectadas en la parte superior de la red.\n","                   pooling=None, # Significa que la salida del modelo será la salida del tensor 4D del último bloque convolucional.\n","                   )\n","\n","modelo_vgg.trainable=False\n","\n","# Estructura de la red\n","modelo_vgg.summary()\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"la7Vmhotdqxc"},"outputs":[],"source":["# Se importa Dense,  Dropout, Flatten de la librería keras.layers\n","# Se importa Conv2D, MaxPooling2D  de la librería keras.layers.convolutional\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers.convolutional import Conv2D, MaxPooling2D\n","from keras.models import Sequential\n","\n","# Se crea el objeto model1 a partir de la clase Sequential()\n","model = Sequential()\n","\n","# Incluir al modelo la red vgg_base traida de VGG16\n","model.add(modelo_vgg)\n","\n","# Ahora agregaremos más pares de capas CONV + POOL a fin de reducir más la\n","# dimensión de la imagen antes de llegar a la capa flatten\n","# Otra estrategia es ir aumentando la cantidad de filtros a medida que crece\n","# la profundidad de la red\n","\n","# Capa de comunicación entre la red convolucional y la red neuronal\n","model.add(Flatten())\n","\n","# Red Neuronal que inicia con 128 neuronas y la función de activación \"relu\"\n","model.add(Dense(units=32, activation='relu'))\n","# Se agrega una capa de dropout para dormir parte de las neuronas.\n","model.add(Dropout(rate=0.2))\n","\n","# Red Neuronal que inicia con 64 neuronas y la función de activación \"relu\"\n","model.add(Dense(units=16, activation='relu'))\n","model.add(Dropout(rate=0.2))\n","\n","# Capa de salida con la cantidad de vegetales y 'softmax' porque es multicategorical\n","model.add(Dense(units=out_shape, activation='softmax'))\n","\n","# Configuración del modelo para el entrenamiento, implementando el método compile a partir del modelo creado.\n","# Se necesita indicar los parámetros:\n","# optimizer, nombre del optimizador (es el algoritmo que se encarga del descenso de gradiente estocástico)\n","# Fuente: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n","# loss, se llama función de pérdida, representa las categorías conocidas de las predicción. Al ser 'categorical_crossentropy' \n","#la predicción tiene una salida con varias opciones.\n","# metrics, se define la métrica que evaluará el modelo durante el entrenamiento y las pruebas.\n","model.compile(optimizer=\"Adam\",\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzgXLNuVeJ7s"},"outputs":[],"source":["# Se entrena el modelo con el método fit\n","# Necesita definir los valores para train_generator, la cantidad de épocas que seria la iteraciones de entrenamiento y\n","# steps_per_epoch, cantidad de imágenes a consumir la red por época.\n","# validation_steps, cantidad de imágenes para validación\n","# validation_data, imagénes preprocesadas.\n","history = model.fit(\n","      train_generator,\n","      steps_per_epoch=epoch_train,\n","      validation_data=validation_generator,\n","      validation_steps=epoch_validation,\n","      epochs=2\n","      )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPnuzSiees61"},"outputs":[],"source":["# Variable epoch_count, que almacena en una lista la cantidad de épocas de train\n","# history, es la variable que almacena las predicciones del modelo\n","# y de ella, se puede acceder a información como su historial (history) del accuracy y val_accuracy\n","epoch_count = range(1, len(history.history['accuracy']) + 1)\n","\n","# De Seaborn (sns) se accede al gráfico de línea para representar el 'accuracy' y val_accuracy\n","sns.lineplot(x=epoch_count,  y=history.history['accuracy'], label='train')\n","sns.lineplot(x=epoch_count,  y=history.history['val_accuracy'], label='val')\n","plt.show()"]},{"cell_type":"code","source":["# Crear el generador de test\n","test_dir = \"./Vegetable Images/test\"\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","test_generator = test_datagen.flow_from_directory(\n","        directory=test_dir,\n","        target_size=(224, 224),\n","        batch_size=3000,  # leemos todas las imagenes juntas de la carpeta test\n","        class_mode=\"categorical\")"],"metadata":{"id":"-XbHE3o8gRz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predecir los datos\n","# Hay que primero leer las imagenes y sus lables del generador\n","batch_imagenes, batch_clases = test_generator.next()\n","\n","# Predecir la salida del modelo\n","y_hat_prob = model.predict(batch_imagenes)\n","\n","# Obtener el \"y\" verdadero --> Almacenar los resultados reales esperados\n","y_test = np.argmax(batch_clases, axis=1)\n","y_test"],"metadata":{"id":"AMCtP3z2gV25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Para la probabilidad de la primer imagen, se ubica su ubicación (Pero no tenemos el nombre del vegetal)\n","y_hat = np.argmax(y_hat_prob,axis=1)\n","y_hat"],"metadata":{"id":"2_3Ru35HgeP4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ¿Cómo obtener el nombre del vegetal de la predicción?"],"metadata":{"id":"9Pn0jC21OjLl"}},{"cell_type":"markdown","source":["#### **Nota:** Los nombres de los vegetales de test_generator.filenames tienen barra, extensiones. Por lo que, hay que extraer solo el nombre."],"metadata":{"id":"aAYBNiMYOtcW"}},{"cell_type":"code","source":["# Muy rebuscada esta forma de obtener los nombres de los vegetales!\n","# Pero en general cuando tenemos los datos de test no tenemos los nombres\n","# por lo que no tenemos el \"y\" verdadero\n","\n","vegetales_test = []\n","\n","# Bucle que recorre todos los nombres de los vegetales de test_generator\n","# Para extraer sólo el nombre\n","for file in test_generator.filenames:\n","\n","    # Una vez ubicado el nombre de la img,\n","    # separa los elementos por \"/\" Tomato/1134.jpg  Ejemplo: ['Tomato', '1142.jpg']\n","    image_name_split = file.split(\"/\") \n","       \n","    # Extrae el primer elemento que corresponde al nombre del vegetal\n","    vegetal_name_split = image_name_split[0]\n","    print(vegetal_name_split )\n","        \n","    # Agrega el nombre del vegetal en una lista \n","    vegetales_test.append(vegetal_name_split)\n","    \n","vegetales_test"],"metadata":{"id":"PZSqXvSQOxiv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIK0ywK0fz5v"},"source":["# Validar modelo\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline5.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"code","source":["# Calcular la exactitud (accuracy)\n","from sklearn.metrics import accuracy_score\n","accuracy_score(y_test, y_hat)"],"metadata":{"id":"Wjr4BTRqfmD2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Se utiliza la matriz de confusión para evaluar la precisión de una clasificación.\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","# Necesita dos variables que contengan los valores a comparar\n","cm = confusion_matrix(y_test, y_hat)\n","\n","# Código para realizar la representación gráfica con los resultados\n","# Se crea la varible cmd, que almacena visualization de la Confusion Matrix \n","# Necesita la variable cm que contiene los resultados de la comparación entre los valores reales y predicción\n","# display_labels, se especifica las etiquetas de las categorias que se evalúan.\n","cmd = ConfusionMatrixDisplay(cm)\n","\n","# Con cmd.plot se especifica el mapa de colores reconocido por matplotlib.\n","cmd.plot(cmap=plt.cm.Blues)\n","\n","# Mostrar la figura\n","plt.show()                                                                                "],"metadata":{"id":"18_ORhWlgIwZ"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyN9ilfd725D+vget8jxUs93"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}